{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7f4a29-02e0-4fbe-b644-06a8ec99626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Bidirectional, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a37904-3655-4272-b6cf-6c09bb0af7bf",
   "metadata": {},
   "source": [
    "# ðŸˆ Predicting Blitzes Using Pre-Snap Behavior\n",
    "\n",
    "**By:** Christopher Doyle, Hans Elasri, Thomas Garity, Rishi Hazra, and Christopher RuaÃ±o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bceb6a-32b1-48f5-8929-cf8ce6af3a2f",
   "metadata": {},
   "source": [
    "## Final Model Pipeline\n",
    "\n",
    "**Here is a high-level overview of the pipeline. Continue scrolling to see a basic implementation with more detail on assumptions and design choices.**\n",
    "\n",
    "--\n",
    "\n",
    "For our final model, we will adopt an RNN architecture. While some elements (like the hyperparameters, specific architecture, and feature selection) will be determined as we iterate, we will begin with a baseline model and then iteratively improve it.\n",
    "\n",
    "We will iterate through the following approaches:\n",
    "1. **Baseline RNN**: Feed through sequences of N rows of data. Each row corresponds to one play, and sequence is made up of N consecutive plays. The final play is the target -- the model must predict whether or not a blitz occurred in the final play.\n",
    "2. **Frame-by-Frame RNN:** Feed through sequences of N rows of data. Each row corresponds to one frame of a single play. The final frame is the target -- the model must predict whether or not a blitz occurred within this play. \n",
    "3. **Mixture of Both:** Run both types of RNN. Combine the hidden states before a MLP head predicts the final blitz / no blitz output.\n",
    "\n",
    "Our Pipeline will be as follows:\n",
    "\n",
    "1. **PREPROCESSING**\n",
    "   1. One hot encode categorical variables (teams, positions, formations)\n",
    "   2. Drop columns that are un-usable.\n",
    "   3. Create sequences, using a sliding window. Write a function for creating sequences -- we want the sliding window size to be flexible (we may want to change this later)\n",
    "2. **MODELING**\n",
    "   1. Define the architecture; the input should be N * (sequence length) * (number of features)\n",
    "   2. Work with some sort of RNN units -- either RNN, GRU, or LSTM\n",
    "   3. Output of final dense layer should be one logits with sigmoid activation for binary classification.\n",
    "   4. We will minimize the binary cross-entropy loss -- this is the most logical approach, as we have chosen to have 1 logit.\n",
    "3. **TRAINING**\n",
    "   1. Train the model on the training data. Log loss, accuracy, and validation accuracy\n",
    "   2. Plot training results over each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969500f5-3e83-467b-a205-2135d5b6d2e8",
   "metadata": {},
   "source": [
    "### Preprocessing \n",
    "Using similar logic to the logistic regression, we:\n",
    "- one-hot encode categorical variables (teams, positions, formations)\n",
    "- cast boolean variables to integers\n",
    "- drop columns that are un-usable\n",
    "- fill Nans with 0 -- these mainly correspond to boolean variables for players whose positions do not apply (i.e. defensive stats for an offensive player), so zeroes are appropriate\n",
    "\n",
    "To obtain our target label, we merge in from the `blitz_outcome` df. In our next imeplementation, we would save those blitz labels to the .csv file itself.\n",
    "\n",
    "The next step is creating sequences. This is the format ready for the model. We have begun with a sequence length of 5, as this allows us to summarize any trends in the current drive, while also peeking at the previous drive as well. In the future we might experiment with:\n",
    "- very large (30+) sequence lengths to capture multiple possessions from both teams\n",
    "- recreate this logic at the frame-level; so we are looking more at real-time decisions (this is closer to our problem statement)\n",
    "\n",
    "\n",
    "NOTE: We have cut off our dataset at only 2 games here. Change the `cutoff` argument to `None` to use the full dataset.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b572fd95-27bb-40a7-860c-dc79538c80a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataframe\n",
    "df = pd.read_csv('nontime_data.csv')\n",
    "blitz_outcome = pd.read_csv('blitz_outcome.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e63cd78-df9e-4fe1-b680-ed7a6ade90ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the existing mapping of plays --> blitzes to add the target column\n",
    "df = df.merge(blitz_outcome[['gameId', 'playId', 'blitzOutcome']], \n",
    "              on=['gameId', 'playId'], \n",
    "              how='left'\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc12c4d9-f77c-4f58-b25e-8ee0163d8a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill all missing categorical fields\n",
    "categorical_cols = []\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    if col not in ['gameId', 'playId', 'nflId']:\n",
    "        categorical_cols.append(col)\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].fillna('Unknown')\n",
    "\n",
    "# one-hot encode categorical cols\n",
    "df = pd.get_dummies(\n",
    "    df,\n",
    "    columns=categorical_cols,\n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "# drop any remaining object-type columns from X\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    df = df.drop(columns=[col])\n",
    "\n",
    "# Convert boolean columns to integers (0/1) first\n",
    "bool_cols = df.select_dtypes(include=['bool']).columns\n",
    "for col in bool_cols:\n",
    "    df[col] = df[col].astype(int)\n",
    "\n",
    "# Drop remaining rows with any NaN\n",
    "df = df.dropna(subset=['blitzOutcome', 'quarter', 'down', 'yardsToGo', 'yardlineNumber', 'gameClock', 'preSnapHomeScore', 'preSnapVisitorScore', 'absoluteYardlineNumber', 'preSnapHomeTeamWinProbability', 'preSnapVisitorTeamWinProbability', 'expectedPoints'])\n",
    "\n",
    "# Fill remaining NaNs with 0\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d72385c-06ca-4998-82fc-5482c6139156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(df, n=5, target_col='blitzOutcome', step=1, cutoff=2):\n",
    "    \"\"\"\n",
    "    Create sequences of n consecutive plays for RNN input with overlapping windows.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame with last frame of each play\n",
    "    n : Sequence length (number of plays to include in each sequence)\n",
    "    target_col : Column name for the target variable (blitz indicator)\n",
    "    step : Step size for sliding window (1 = maximum overlap, n = no overlap)\n",
    "    cutoff : how many games to repeat this process for (for prototyping)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X : numpy array of shape (num_sequences, n, num_features)\n",
    "        Sequences of n plays with features\n",
    "    y : numpy array of shape (num_sequences,)\n",
    "        Target values indicating whether the n+1th play was a blitz\n",
    "    play_ids : list of tuples\n",
    "        Identifiers for the play following each sequence (for reference)\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    play_ids = []\n",
    "    \n",
    "    # list of unique games\n",
    "    games = df['gameId'].unique()\n",
    "    \n",
    "    # Handle cutoff=None\n",
    "    if cutoff is None:\n",
    "        cutoff = len(games)\n",
    "\n",
    "    # Print number of games you are sampling from\n",
    "    print(f'Sampling from {cutoff} games')\n",
    "\n",
    "    for game_id in games[:cutoff]:\n",
    "        # Get plays for this game and sort chronologically\n",
    "        game_plays = df[df['gameId'] == game_id].sort_values(['quarter', 'gameClock'], ascending=[True, False])\n",
    "        \n",
    "        # Get the length of this game in plays\n",
    "        game_length = len(game_plays)\n",
    "        \n",
    "        # Skip games that are too short for our sequence length\n",
    "        if game_length <= n:\n",
    "            continue\n",
    "            \n",
    "        # Specify which features to use\n",
    "        feature_cols = [col for col in df.columns if col not in ['gameId', 'playId', 'blitzOutcome']]\n",
    "        \n",
    "        # Convert to numpy for faster operations\n",
    "        plays_array = game_plays[feature_cols].values\n",
    "        targets_array = game_plays[target_col].values if target_col in game_plays.columns else None\n",
    "        play_ids_array = game_plays['playId'].values\n",
    "\n",
    "        # Create overlapping windows\n",
    "        for i in range(0, game_length - n, step):\n",
    "            # Get n consecutive plays for X\n",
    "            sequence = plays_array[i:i+n]\n",
    "            \n",
    "            # Skip sequences with NaN values if needed\n",
    "            # Although we should not have any at this point\n",
    "            if np.isnan(sequence).any():\n",
    "                # Flag it so we can debug\n",
    "                print('skipping')\n",
    "                continue\n",
    "                \n",
    "            # Add the sequence to our dataset\n",
    "            X.append(sequence)\n",
    "            \n",
    "            # Get target from the n+1th play (if target column exists)\n",
    "            if targets_array is not None:\n",
    "                y.append(targets_array[i+n])\n",
    "                \n",
    "            # Keep track of which play this prediction is for\n",
    "            # This is mainly for interpretability\n",
    "            play_ids.append((game_id,play_ids_array[i+n]))\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y) if targets_array is not None else None\n",
    "    \n",
    "    print(f\"Created {len(X)} sequences of length {n}\")\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    if y is not None:\n",
    "        print(f\"y shape: {y.shape}\")\n",
    "    \n",
    "    return X, y, play_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9097147-4b9b-4e4e-81ca-4d5b1f1318b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sequences\n",
    "X, y, play_ids = create_sequences(df, n=5, target_col='blitzOutcome', step=1, cutoff=None)\n",
    "\n",
    "# Split into 80% training and 20% validation and testing\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Take leftover 20% and split into 10% validation and 10% test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f4c0ef-6598-4a26-a3aa-7ddadd69e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each label\n",
    "labels, counts = np.unique(y_train, return_counts=True)\n",
    "label_names = ['No blitz', 'Blitz']\n",
    "\n",
    "# Calculate percentages\n",
    "total = sum(counts)\n",
    "percentages = [count / total * 100 for count in counts]\n",
    "\n",
    "# Plot\n",
    "plt.bar(label_names, counts)\n",
    "plt.title('Class imbalance in training data')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add percentage labels on top of bars\n",
    "for i, (count, pct) in enumerate(zip(counts, percentages)):\n",
    "    plt.text(i, count + total * 0.01, f'{pct:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d181474b-1b7a-400e-8415-fa937e5d2ebc",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "This is roughly in-line with our % of blitzes across all plays -- so we can suggest that we are sampling correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016fdd00-0481-484d-b97a-d07464227510",
   "metadata": {},
   "source": [
    "### Define RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ba896d-09ef-463a-b76c-5c88121d46ee",
   "metadata": {},
   "source": [
    "We have not put extreme thought into this model yet -- that will be work for Milestone 5. However, with this working scaffolding, we can begin to iterate. Some design decisions we have made already are:\n",
    "- **Tracking precision / recall** -- we know that blitzes are rare, so it is important that we know whether we are correctly identifying them (not just predicting the majority class)\n",
    "- **Bidirectional LSTM layers** -- we were impressed by LSTMs in psets so have chosen them as the starting point. We have chosen bidirectional thinking that football strategies are somewhat bidirectional as well ; a defensive coordinator both looks back and previous plays and plans for potential future plays to lead their decisions. We need to test this logic further through experiments with regular and bidirectional plays.\n",
    "- **Dropout** -- there is a risk of overfitting with this type of task (very noisy, small dataset), so we have added dropout to prevent overfitting.\n",
    "- **Early Stopping** -- we have added early stopping to prevent overfitting.\n",
    "- **Batch Normalization** -- same as above ; one more measure to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40946f4-e9dc-4b13-8b26-c32661f1ed74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blitz_rnn_model(n_timesteps, n_features, dropout_rate=0.3, lstm_units=256):\n",
    "    model = Sequential([\n",
    "        # Input layer\n",
    "        Input(shape=(n_timesteps, n_features)),\n",
    "        \n",
    "        # Bidirectional LSTM layer\n",
    "        Bidirectional(LSTM(256, return_sequences=True)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        # Second bidirectional LSTM layer\n",
    "        Bidirectional(LSTM(256)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        # Dense hidden layer\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "\n",
    "        # Dense hidden layer\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        # Output layer with 2 neurons (probability of blitz and no blitz)\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Compile model with binary cross-entropy loss\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=['accuracy', 'AUC', 'Precision', 'Recall']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8c2abc-7116-4e04-a066-78137d5dda81",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_model = create_blitz_rnn_model(n_timesteps=X.shape[1], n_features=X.shape[2], dropout_rate=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2008604-8fdf-4aaa-9e35-c4dcf1025e65",
   "metadata": {},
   "source": [
    "### Training Run\n",
    "\n",
    "Keep in mind, this is a demo -- we would run this for more epochs and tune our hyperparameters for our final model.\n",
    "\n",
    "Be sure to update the # of epochs to be greater than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884969af-5077-4f5c-94b3-8be6c29a75f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_recall', patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001),\n",
    "    ModelCheckpoint('best_blitz_model.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "# Use class weighting to adjust for massive class imbalance:\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Compute weights\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "\n",
    "history = RNN_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    class_weight=class_weights,\n",
    "    epochs=25,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd59ce11-2a29-46e3-b4a9-72b586efd793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_metrics(history):\n",
    "    metrics = ['accuracy', 'precision', 'recall']\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.plot(history.history[metric], label=f'Train {metric}')\n",
    "        plt.plot(history.history[f'val_{metric}'], label=f'Val {metric}')\n",
    "        plt.title(metric.capitalize())\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(metric)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73945170-de86-4805-8e32-34fd5ef4edab",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2fafb5-a4d9-47d4-a164-58b10189547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6db1415-3779-437c-900e-cf983986723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Logs\n",
    "loss, accuracy, auc, precision, recall = RNN_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test AUC: {auc:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea45042-eedf-4e1d-997e-fb1f950a9f20",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "The model predicts with a test accuracy of 66%, and a test recall of 30%. This means that the model predicts 1 in 3 blitzes -- this could be useful after all!\n",
    "\n",
    "Some design decisions thus far are:\n",
    "- Class weights -- this was a game changer\n",
    "- LSTM layers -- bidirectional, since previous plays affect future ones, and potential future plays affect current ones (this may be too-flimsy logic)\n",
    "- Early stopping based on recall -- as accuracy climbs, the recall tends to drop off (in early epochs, the model predicts too many blitzes)\n",
    "\n",
    "However, there are a few design choices we can evaluate further:\n",
    "- Scaling the dataset in preprocessing -- right now, we do nothing\n",
    "- BinaryFocalCrossentropy loss did not help -- although maybe it could be worthwhile\n",
    "- We don't know what the ideal input data looks like: is it better to feed through loads of plays, and let the model draw long-term predictions, or should we draw in shorter sequences?\n",
    "- Same goes for step size in the sampling function\n",
    "- Are we sure we have the right level of dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00a4d38-1e34-4a3e-9ba7-4379382f0243",
   "metadata": {},
   "source": [
    "## Input Feature Scaling\n",
    "\n",
    "We perform an experiment to test for the best approach to scaling:\n",
    "- Scale to [0, 1] -- many values are indicators on that interval anyways\n",
    "- Standard scaler -- this likely will not work, since our data does not assume a gaussian distribution\n",
    "- Scale to [-1, 1] -- the `tanh` activations in the LSTM layers may benefit from this\n",
    "- No scaling (control) -- this is what we have currently -- and results have been okay (30% test recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dac03b-d8d2-449f-a503-b52c988933a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "def test_scaler(X, y, results=None, scaler='None', n_trials=5):\n",
    "    # Scale (or do not scale)\n",
    "\n",
    "    if scaler == 'None':\n",
    "        X_scaled = X\n",
    "\n",
    "    else:\n",
    "        # Manipulate data to be 2D (most scalers expect 2D data)\n",
    "        # Suppose X shape is (n_samples, n_timesteps, n_features)\n",
    "        n_samples, n_timesteps, n_features = X.shape\n",
    "        \n",
    "        # Reshape to 2D for scaling (merge samples and timesteps)\n",
    "        X_2d = X.reshape(-1, n_features)\n",
    "\n",
    "        if scaler == '[0,1]':\n",
    "            # Normalize features to [0, 1] (handles mixed scales safely)\n",
    "            scaler = MinMaxScaler()\n",
    "    \n",
    "        if scaler == '[-1,1]':\n",
    "            scaler = MinMaxScaler(feature_range=(-1, 1))    \n",
    "    \n",
    "        if scaler == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "\n",
    "        # Fit and transform\n",
    "        X_scaled_2d = scaler.fit_transform(X_2d)\n",
    "                \n",
    "        # Reshape back to original 3D shape\n",
    "        X_scaled = X_scaled_2d.reshape(n_samples, n_timesteps, n_features)\n",
    "\n",
    "    # Split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    for i in range(n_trials):\n",
    "        print(f'Scaler: {scaler}, Trial: {i}')\n",
    "        # Create model\n",
    "        RNN_model = create_blitz_rnn_model(n_timesteps=X.shape[1], n_features=X.shape[2], dropout_rate=0.2)\n",
    "    \n",
    "        # Train and evaluate\n",
    "        # Compute weights\n",
    "        class_weights = class_weight.compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=np.unique(y_train),\n",
    "            y=y_train\n",
    "        )\n",
    "        class_weights = dict(enumerate(class_weights))\n",
    "        \n",
    "        # Train\n",
    "        history = RNN_model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_test, y_test),\n",
    "            class_weight=class_weights,\n",
    "            epochs=10,\n",
    "            batch_size=32,\n",
    "        )\n",
    "    \n",
    "        # Evaluate and save\n",
    "        results = evaluate_and_save(RNN_model, X_test, y_test, ['scaler', scaler], i, results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234cda89-323a-4e10-b0ec-bc108bea11a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = None\n",
    "# Run the experiment for each type of scaler 5 times\n",
    "for scaler in ['standard', 'None', '[0,1]', '[-1,1]']:\n",
    "    # Run experiment\n",
    "    results = test_scaler(X, y, results=results, scaler=scaler, n_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8133b0aa-62e6-4d05-be0b-cd8817ce399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results across different scalers\n",
    "\n",
    "# Sort for consistent plotting\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=results_df, x='scaler', y='recall')\n",
    "plt.title('Recall by Scaler Type')\n",
    "plt.ylabel('Recall')\n",
    "plt.xlabel('Scaler')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a848aa-ee31-4e58-86d1-b99ae2908e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bcfad4-83e7-41ef-9e99-820a5ebc5aa4",
   "metadata": {},
   "source": [
    "## Sequence Tuning\n",
    "\n",
    "Here, we tune the parameters that control how sequences are constructed from the play-level data. This affects how much historical context the model sees, and how densely overlapping the training examples are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2694855-1d62-4ab8-9b53-32f2b997213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used in experiments to create a df (first as a dict) of results\n",
    "# var : {string, value} is the independent variable,\n",
    "# where var[0] is a string label for the variable\n",
    "import os\n",
    "\n",
    "def evaluate_and_save(model, X_test, y_test, var, trial, results=None):\n",
    "    # Evaluate\n",
    "    loss, accuracy, auc, precision, recall = model.evaluate(X_test, y_test)\n",
    "\n",
    "    if results == None:\n",
    "        # Set up results dict\n",
    "        results =  {'loss':[],\n",
    "                    'accuracy':[],\n",
    "                    'auc':[],\n",
    "                    'precision':[],\n",
    "                    'recall':[],\n",
    "                    var[0]:[]}\n",
    "\n",
    "    # Save results\n",
    "    results['loss'].append(loss)\n",
    "    results['accuracy'].append(accuracy)\n",
    "    results['auc'].append(auc)\n",
    "    results['precision'].append(precision)\n",
    "    results['recall'].append(recall)\n",
    "    results[var[0]].append(var[1])\n",
    "\n",
    "    # Set up results directory if it doesn't exist\n",
    "    os.makedirs(f'{var[0]}_experiment', exist_ok=True)\n",
    "\n",
    "    \n",
    "    # Save model\n",
    "    model.save(f'{var[0]}_experiment/{var[1]}_{var[0]}_trial_{trial}.keras')\n",
    "\n",
    "    return results    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abc4e04-24ab-44bf-9b75-b9fb6ee77fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up results dict\n",
    "results = {'steps':[],\n",
    "          'sequence_length':[],\n",
    "          'loss':[],\n",
    "          'accuracy':[],\n",
    "          'precision':[],\n",
    "          'recall':[]}\n",
    "\n",
    "\n",
    "# Iterate over potential sequence lengths\n",
    "for n in range(1, 20, 2):\n",
    "\n",
    "    # Iterate over potential step sizes\n",
    "    for step in range(1, 5):\n",
    "\n",
    "        # Sample data\n",
    "        # Prepare sequences\n",
    "        X, y, play_ids = create_sequences(df, n=n, target_col='blitzOutcome', step=step, cutoff=None)\n",
    "        # Split \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Create model\n",
    "        RNN_model = create_blitz_rnn_model(n_timesteps=X.shape[1], n_features=X.shape[2], dropout_rate=0.2)\n",
    "\n",
    "        # Train and evaluate\n",
    "        # Compute weights\n",
    "        class_weights = class_weight.compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=np.unique(y_train),\n",
    "            y=y_train\n",
    "        )\n",
    "        class_weights = dict(enumerate(class_weights))\n",
    "        \n",
    "        # Train\n",
    "        history = RNN_model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_test, y_test),\n",
    "            class_weight=class_weights,\n",
    "            epochs=25,\n",
    "            batch_size=32,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        # Evaluate and save\n",
    "        results = evaluate_and_save(RNN_model, X_test, y_test, n, step, results=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eefbf73-0f87-41b8-bc4e-59c5ecef6ce7",
   "metadata": {},
   "source": [
    "### Dropout Rate and LSTM Units\n",
    "We tune dropout and LSTM units together to manage the trade-off between model capacity and overfitting in learning blitz patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8603f5c-b1a4-4eb8-99bd-d040897b362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter grid\n",
    "dropout_rates = [0.1, 0.2, 0.3, 0.4]\n",
    "lstm_units_list = [128, 256, 512]\n",
    "n_trials = 3\n",
    "\n",
    "# loop thru combos\n",
    "results = None\n",
    "trial = 0\n",
    "\n",
    "for dropout_rate in dropout_rates:\n",
    "    for lstm_units in lstm_units_list:\n",
    "        for i in range(n_trials):\n",
    "\n",
    "            print(f'Trial {i+1}/{n_trials} â€” Dropout: {dropout_rate}, LSTM Units: {lstm_units}')\n",
    "\n",
    "            # build model\n",
    "            model = create_blitz_rnn_model(\n",
    "                n_timesteps=X.shape[1],\n",
    "                n_features=X.shape[2],\n",
    "                dropout_rate=dropout_rate,\n",
    "                lstm_units=lstm_units\n",
    "            )\n",
    "\n",
    "            # compile\n",
    "            model.compile(\n",
    "                optimizer=Adam(learning_rate=0.001),\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                metrics=['accuracy', 'AUC', 'Precision', 'Recall']\n",
    "            )\n",
    "\n",
    "            # class weights\n",
    "            class_weights = class_weight.compute_class_weight(\n",
    "                class_weight='balanced',\n",
    "                classes=np.unique(y_train),\n",
    "                y=y_train\n",
    "            )\n",
    "            class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "            # train\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_test, y_test),\n",
    "                class_weight=class_weights,\n",
    "                epochs=10,\n",
    "                batch_size=32,\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            # save results\n",
    "            tag = f'dropout{dropout_rate}_lstm{lstm_units}'\n",
    "            results = evaluate_and_save(\n",
    "                model,\n",
    "                X_test,\n",
    "                y_test,\n",
    "                var=('config', tag),\n",
    "                trial=trial,\n",
    "                results=results\n",
    "            )\n",
    "            trial += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1212256a-60f0-40f4-b8a1-c5d0edf0be40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(results, tuning_param):\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    if tuning_param not in results_df.columns:\n",
    "        raise ValueError(f\"'{tuning_param}' not found in results.\")\n",
    "\n",
    "    summary = results_df.groupby(tuning_param).agg({\n",
    "        'recall': ['mean', 'std'],\n",
    "        'precision': ['mean', 'std'],\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'auc': ['mean', 'std'],\n",
    "        'loss': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "\n",
    "    summary.columns = [tuning_param, 'recall_mean', 'recall_std',\n",
    "                       'precision_mean', 'precision_std',\n",
    "                       'accuracy_mean', 'accuracy_std',\n",
    "                       'auc_mean', 'auc_std',\n",
    "                       'loss_mean', 'loss_std']\n",
    "\n",
    "    return summary.sort_values(by='recall_mean', ascending=False)\n",
    "\n",
    "summary = summarize_results(results, tuning_param='config')\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449540ae-2e05-4b87-bd25-2df527621796",
   "metadata": {},
   "source": [
    "## Learning Rate\n",
    "We experiment with learning rates to find a balance between fast convergence and stable, accurate blitz predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81269dd-f244-4581-8d80-517dfb2ea6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lrs to test\n",
    "learning_rates = [0.01, 0.001, 0.0005, 0.0001]\n",
    "n_trials = 5\n",
    "\n",
    "# loop thru lrs\n",
    "results = None\n",
    "trial = 0\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for i in range(n_trials):\n",
    "        print(f'Trial {i+1}/{n_trials} â€” Learning rate: {lr}')\n",
    "\n",
    "        # create model\n",
    "        model = create_blitz_rnn_model(\n",
    "            n_timesteps=X.shape[1],\n",
    "            n_features=X.shape[2],\n",
    "            dropout_rate=0.3,\n",
    "            lstm_units=256\n",
    "        )\n",
    "\n",
    "        # compile\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=lr),\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "            metrics=['accuracy', 'AUC', 'Precision', 'Recall']\n",
    "        )\n",
    "\n",
    "        # class weights\n",
    "        class_weights = class_weight.compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=np.unique(y_train),\n",
    "            y=y_train\n",
    "        )\n",
    "        class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "        # train\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_test, y_test),\n",
    "            class_weight=class_weights,\n",
    "            epochs=10,\n",
    "            batch_size=32,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # log and save\n",
    "        tag = f'lr{lr}'\n",
    "        results = evaluate_and_save(\n",
    "            model,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            var=('lr', tag),\n",
    "            trial=trial,\n",
    "            results=results\n",
    "        )\n",
    "        trial += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183349ce-b43d-4db4-a9e5-a76d525808e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output results\n",
    "summary = summarize_results(results, tuning_param='lr')\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92c8bc7-d234-4bcf-8adc-bc88fef32825",
   "metadata": {},
   "source": [
    "## Binary Focal Loss\n",
    "We use binary focal loss to address class imbalance by making the model focus more on misclassified examples rather than defaulting to the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2de72a-8cd9-44b7-9703-3a7ae2033cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import BinaryCrossentropy, BinaryFocalCrossentropy\n",
    "\n",
    "# loss fns to compare\n",
    "loss_types = {\n",
    "    'bce': BinaryCrossentropy(),\n",
    "    'focal': BinaryFocalCrossentropy(\n",
    "        gamma=2.0,\n",
    "        alpha=0.25,\n",
    "        from_logits=False,\n",
    "        name='binary_focal_crossentropy'\n",
    "    )\n",
    "}\n",
    "\n",
    "# loop thru loss fns\n",
    "n_trials = 5\n",
    "results = None\n",
    "trial = 0\n",
    "\n",
    "for loss_name, loss_fn in loss_types.items():\n",
    "    for i in range(n_trials):\n",
    "        print(f'Trial {i+1}/{n_trials} â€” Loss: {loss_name}')\n",
    "\n",
    "        model = create_blitz_rnn_model(\n",
    "            n_timesteps=X.shape[1],\n",
    "            n_features=X.shape[2],\n",
    "            dropout_rate=0.3,\n",
    "            lstm_units=256\n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.0005),\n",
    "            loss=loss_fn,\n",
    "            metrics=['accuracy', 'AUC', 'Precision', 'Recall']\n",
    "        )\n",
    "\n",
    "        # Compute class weights for imbalance\n",
    "        class_weights = class_weight.compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=np.unique(y_train),\n",
    "            y=y_train\n",
    "        )\n",
    "        class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "        callbacks = [EarlyStopping(monitor='val_recall', patience=5, restore_best_weights=True)]\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_test, y_test),\n",
    "            class_weight=class_weights,\n",
    "            epochs=40,\n",
    "            batch_size=32,\n",
    "            callbacks=callbacks,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        results = evaluate_and_save(\n",
    "            model,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            var=('loss_fn', loss_name),\n",
    "            trial=trial,\n",
    "            results=results\n",
    "        )\n",
    "\n",
    "        trial += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20e6d92-6afd-4113-ad4c-2e30b2e6a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output results\n",
    "summary = summarize_results(results, tuning_param='loss_fn')\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ab2af3-2ed8-4070-9b5b-3c44a1b11597",
   "metadata": {},
   "source": [
    "## Model Architecture  \n",
    "We explore different model architectures to evaluate whether we can improve recall or achieve comparable performance with simpler, more efficient designs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ac1ad5-03b2-4053-9e37-ddc7d0e5fcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_variant(n_timesteps, n_features, \n",
    "                       lstm_units=256, \n",
    "                       dropout_rate=0.3, \n",
    "                       bidirectional=True, \n",
    "                       num_lstm_layers=2, \n",
    "                       dense_width=512):\n",
    "    \n",
    "    def lstm_layer(return_sequences=False):\n",
    "        base = LSTM(lstm_units, return_sequences=return_sequences)\n",
    "        return Bidirectional(base) if bidirectional else base\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(n_timesteps, n_features)))\n",
    "    \n",
    "    model.add(lstm_layer(return_sequences=(num_lstm_layers == 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    if num_lstm_layers == 2:\n",
    "        model.add(lstm_layer(return_sequences=False))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(dense_width, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(dense_width, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=['accuracy', 'AUC', 'Precision', 'Recall']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a8667f-62cf-4a39-9240-d812b9a3c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# architecture grid\n",
    "lstm_units = 256\n",
    "\n",
    "architectures = list(itertools.product(\n",
    "    [256, 512],      # Dense width\n",
    "    [True, False],   # Bidirectional\n",
    "    [1, 2]           # LSTM layers\n",
    "))\n",
    "\n",
    "n_trials = 3\n",
    "results = None\n",
    "trial = 0\n",
    "\n",
    "for dense_width, bidirectional, num_lstm_layers in architectures:\n",
    "    for i in range(n_trials):\n",
    "        config_name = f\"lstm{lstm_units}_dense{dense_width}_{'bi' if bidirectional else 'uni'}_{num_lstm_layers}L\"\n",
    "        print(f\"Trial {i+1}/{n_trials} â€” {config_name}\")\n",
    "\n",
    "        model = create_rnn_variant(\n",
    "            n_timesteps=X.shape[1],\n",
    "            n_features=X.shape[2],\n",
    "            lstm_units=lstm_units,\n",
    "            dense_width=dense_width,\n",
    "            bidirectional=bidirectional,\n",
    "            num_lstm_layers=num_lstm_layers\n",
    "        )\n",
    "\n",
    "        class_weights = class_weight.compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=np.unique(y_train),\n",
    "            y=y_train\n",
    "        )\n",
    "        class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_test, y_test),\n",
    "            class_weight=class_weights,\n",
    "            epochs=10,\n",
    "            batch_size=32,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        results = evaluate_and_save(\n",
    "            model,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            var=('architecture', config_name),\n",
    "            trial=trial,\n",
    "            results=results\n",
    "        )\n",
    "        trial += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84fd430-9923-405f-b1c7-e3451cc64f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summarize_results(results, tuning_param='architecture')\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3434e6-834c-4802-8f3e-a1aebf8c72f7",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5499a52-52eb-498b-b821-767a98401e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plug in final model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb04dff-c0ff-46cc-97bf-5bfe41e475e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate final model\n",
    "loss, accuracy, auc, precision, recall = final_model.evaluate(X_test, y_test)\n",
    "print(f\"Final Test Loss: {loss:.4f}\")\n",
    "print(f\"Final Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Final Test AUC: {auc:.4f}\")\n",
    "print(f\"Final Test Precision: {precision:.4f}\")\n",
    "print(f\"Final Test Recall: {recall:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
